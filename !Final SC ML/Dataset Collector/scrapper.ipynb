{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "\n",
    "BASE_IDX = \"https://news.detik.com/indeks\"\n",
    "HEADERS  = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "OUTPUT   = \"detik_full_2-300.csv\"\n",
    "\n",
    "# Regex to find timestamps\n",
    "TS_RE = re.compile(r\"\\d+\\s+(detik|menit|jam|hari)\\s+yang\\s+lalu\", re.I)\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    return text.strip().replace(\"\\r\",\" \").replace(\"\\n\", \" \")\n",
    "\n",
    "def extract_article(link: str) -> dict:\n",
    "    \"\"\"Fetch a Detik article and return its title, url, timestamp, and main content.\"\"\"\n",
    "    r = requests.get(link, headers=HEADERS, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # Title\n",
    "    title_tag = soup.select_one(\"h1.detail__title, h1.entry-title\")\n",
    "    title = clean(title_tag.get_text()) if title_tag else \"\"\n",
    "\n",
    "    # Timestamp\n",
    "    ts = \"\"\n",
    "    meta = soup.select_one(\"div.detail__date, span.date\")\n",
    "    if meta:\n",
    "        m = TS_RE.search(meta.get_text())\n",
    "        if m:\n",
    "            ts = m.group(0)\n",
    "\n",
    "    # Body paragraphs\n",
    "    body_div = soup.select_one(\"div.detail__body-text.itp_bodycontent\") \\\n",
    "           or soup.select_one(\"div.detail__body\")\n",
    "    paragraphs = []\n",
    "    if body_div:\n",
    "        for p in body_div.find_all(\"p\"):\n",
    "            txt = clean(p.get_text())\n",
    "            if not txt:\n",
    "                continue\n",
    "            if txt.lower().startswith(\"baca juga\"):\n",
    "                continue\n",
    "            # skip captions or labels that contain klik in txt\n",
    "            if \"klik\" in txt.lower():\n",
    "                continue\n",
    "            paragraphs.append(txt)\n",
    "    content = \"\\n\".join(paragraphs)\n",
    "\n",
    "    return {\"title\": title, \"url\": link, \"timestamp\": ts, \"content\": content}\n",
    "\n",
    "# Prepare CSV\n",
    "with open(OUTPUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"page\",\"title\",\"url\",\"timestamp\",\"content\"])\n",
    "    writer.writeheader()\n",
    "\n",
    "for page in range(2, 401):\n",
    "    print(f\"→ Scraping index page {page}\")\n",
    "    idx_r = requests.get(BASE_IDX, params={\"page\": page}, headers=HEADERS, timeout=10)\n",
    "    idx_r.raise_for_status()\n",
    "    idx_soup = BeautifulSoup(idx_r.text, \"html.parser\")\n",
    "\n",
    "    # Each index entry\n",
    "    links = [a[\"href\"] for a in idx_soup.select(\"h3 > a[href^='https://news.detik.com/']\")]\n",
    "\n",
    "    page_rows = []\n",
    "    for i, link in enumerate(links, 1):\n",
    "        try:\n",
    "            rec = extract_article(link)\n",
    "            rec[\"page\"] = page\n",
    "            page_rows.append(rec)\n",
    "            print(f\"   • [{i}/{len(links)}] {rec['title']!r}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   [WARN] Failed {link}: {e}\")\n",
    "        time.sleep(0.2) \n",
    "\n",
    "    with open(OUTPUT, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"page\",\"title\",\"url\",\"timestamp\",\"content\"])\n",
    "        writer.writerows(page_rows)\n",
    "\n",
    "print(f\"Finished! All data written to {OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeecef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://turnbackhoax.id\"\n",
    "HEADERS  = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "CSV_FILE = \"hoax_final.csv\"\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    return text.strip().replace(\"\\r\", \"\")\n",
    "\n",
    "def parse_article_legacy(soup: BeautifulSoup) -> dict:\n",
    "    \"\"\"\n",
    "    Parse a single article using the legacy template:\n",
    "      - Title from <h1.entry-title>\n",
    "      - Narasi by splitting on 'NARASI:' / 'Narasi :'\n",
    "      - Penjelasan by splitting on 'PENJELASAN:' / 'Penjelasan :'\n",
    "    \"\"\"\n",
    "    # Title\n",
    "    title_tag = soup.select_one(\"h1.entry-title\")\n",
    "    title = clean(title_tag.get_text()) if title_tag else \"\"\n",
    "\n",
    "    # Full content text\n",
    "    content = soup.select_one(\"div.post-content\") or soup.select_one(\"div.entry-content\")\n",
    "    full_text = content.get_text(\"\\n\") if content else \"\"\n",
    "\n",
    "    # Split on the labels\n",
    "    parts = re.split(\n",
    "        r'(?mi)^(NARASI\\s*:|Narasi\\s*:|PENJELASAN\\s*:|Penjelasan\\s*:)\\s*',\n",
    "        full_text, flags=re.MULTILINE\n",
    "    )\n",
    "    narasi = penjelasan = \"\"\n",
    "    for i in range(1, len(parts), 2):\n",
    "        label = parts[i].lower()\n",
    "        body  = parts[i+1].strip()\n",
    "        if \"narasi\" in label:\n",
    "            narasi = clean(body)\n",
    "        elif \"penjelasan\" in label:\n",
    "            penjelasan = clean(body)\n",
    "    return {\"Title\": title, \"Narasi\": narasi, \"Penjelasan\": penjelasan}\n",
    "\n",
    "def scrape_legacy_range(start_page: int, end_page: int, csv_file: str):\n",
    "    # Write header once\n",
    "    with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"Title\", \"Narasi\", \"Penjelasan\"])\n",
    "        writer.writeheader()\n",
    "\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        idx_url = f\"{BASE_URL}/page/{page}/\"\n",
    "        try:\n",
    "            idx_r = requests.get(idx_url, headers=HEADERS, timeout=10)\n",
    "            idx_r.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Could not load index page {page}: {e}\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "        idx_soup = BeautifulSoup(idx_r.text, \"html.parser\")\n",
    "        links = [\n",
    "            a[\"href\"] for a in idx_soup.select(\"h3.entry-title a[href^='http']\")\n",
    "        ]\n",
    "        print(f\"→ Page {page}: {len(links)} articles found\")\n",
    "\n",
    "        page_rows = []\n",
    "        for i, link in enumerate(links, 1):\n",
    "            try:\n",
    "                art_r = requests.get(link, headers=HEADERS, timeout=10)\n",
    "                art_r.raise_for_status()\n",
    "                art_soup = BeautifulSoup(art_r.text, \"html.parser\")\n",
    "                rec = parse_article_legacy(art_soup)\n",
    "                if rec[\"Narasi\"] or rec[\"Penjelasan\"] or rec[\"Title\"]:\n",
    "                    page_rows.append(rec)\n",
    "                print(f\"   • [{i}/{len(links)}] {rec['Title']!r}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   [WARN] Skipped {link}: {e}\", file=sys.stderr)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        with open(csv_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\"Title\", \"Narasi\", \"Penjelasan\"])\n",
    "            writer.writerows(page_rows)\n",
    "\n",
    "        print(f\"Page {page} done — {len(page_rows)} articles written\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_legacy_range(100, 800, CSV_FILE)\n",
    "    print(\"All done — legacy template scraped to\", CSV_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
